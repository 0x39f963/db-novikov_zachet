{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "377452be-0599-458c-819f-fdfcc7587dfe",
   "metadata": {},
   "source": [
    "# Системы хранения и обработки данных: диф. зачет (письменное задание)\n",
    "## Новиков Иван\n",
    "\n",
    "---\n",
    "\n",
    "# 0. Содержание и артефакты:\n",
    "\n",
    "В репозитории:\n",
    "\n",
    "**0.1 репозиторий (тут оформлен README.md со всеми картинками)**\n",
    "- https://github.com/0x39f963/db-novikov_zachet\n",
    "\n",
    "**0.2 главнгый файл с кодом ДЗ (есть еще sql скрипт дополнительно, для импорта данных)**\n",
    "- https://github.com/0x39f963/db-novikov_zachet/spark/main.py\n",
    "\n",
    "**0.3 ноутбук, который дублирует информацию, которая изложена в README.md**\n",
    "- https://github.com/0x39f963/db-novikov_zachet/NovikovIA.ipynb\n",
    " \n",
    "*(README открывается сразу при переходе в репу, мне кажется, что так удобнее)*\n",
    "\n",
    "**0.4 Содержание:**\n",
    "-  Развертывание: wsl2 / docker / win11\n",
    "-  Скрипт предварительной загрузки данных в postgres и скриншоты выполнения\n",
    "-  Основной код ДЗ и скриншоты выполнения\n",
    "\n",
    ".\n",
    "\n",
    "# 1. Развертывание: wsl2 / docker / win11\n",
    "\n",
    "1.0  Развертывалось в wsl2 / docker (win11). В репозиторий зашли все файлы/конфиги для развертывания.\n",
    "\n",
    "В репозитории имеются \n",
    "docker-compose.yml\n",
    "и \n",
    "./spark/test_spark_1.py\n",
    "./spark/test_spark_2.py\n",
    "\n",
    "**1.1**  Поднимаем контейнеры \n",
    "![1.1](https://github.com/0x39f963/db-novikov_zachet/images/5.png)\n",
    "\n",
    ".\n",
    "\n",
    "**1.2**  Проверяем, что все собралось, порты, версию спарка:\n",
    "![1.2](https://github.com/0x39f963/db-novikov_zachet/images/3.png)\n",
    "\n",
    "\n",
    "**1.3 **  Тестирование обращения к postgre и к mongo из spark \n",
    "\n",
    "*(приведен в т.ч. код тест скриптов, как можно догодаться, завелось все сильно не с первого раза, в т.ч. пришлось подбирать версию коннектора)*\n",
    "**1.3.1 ./spark/test_spark_1.py** \n",
    "\n",
    "```python\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"pg-smoke\").getOrCreate()\n",
    "\n",
    "url = \"jdbc:postgresql://postgres:5432/hw\"\n",
    "props = {\"user\":\"hw\",\"password\":\"hwpass\",\"driver\":\"org.postgresql.Driver\"}\n",
    "\n",
    "df = spark.read.jdbc(url, \"(select 1 as ok) t\", properties=props)\n",
    "df.show()\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "![1.3.1.2](https://github.com/0x39f963/db-novikov_zachet/images/1.png)\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    "**1.3.2 ./spark/test_spark_2.py** \n",
    "\n",
    "```python\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"mongo-smoke\")\n",
    "    .config(\"spark.mongodb.read.connection.uri\", \"mongodb://mongo:27017/hw\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "try:\n",
    "    df = spark.read.format(\"mongodb\").option(\"collection\",\"reviews\").load()\n",
    "    df.printSchema()\n",
    "    print(\"rows:\", df.limit(1).count())\n",
    "except Exception as e:\n",
    "    print(\"Mongo read failed:\", e)\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "![1.3.1.2](https://github.com/0x39f963/db-novikov_zachet/images/2.png)\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    "# 2. Скрипт предварительной загрузки данных в postgres и скриншоты выполнения:\n",
    "\n",
    "**./data/init.sql в репозитории**\n",
    "\n",
    "```sql\n",
    "\n",
    "\\set ON_ERROR_STOP on\n",
    "\\pset pager off\n",
    "\n",
    "BEGIN;\n",
    "\n",
    "SET client_encoding = 'UTF8';\n",
    "\n",
    "-- 1. базовые таблицы + таблица для резульататов\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS orders (\n",
    "\torder_id int4 NOT NULL,\n",
    "    order_date timestamp NOT NULL,\n",
    "    customer_id int4 NOT NULL,\n",
    "    CONSTRAINT pk_orders PRIMARY KEY  (order_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS order_items (\n",
    "    order_id int4 NOT NULL,\n",
    "    product_id int4 NOT NULL,\n",
    "    quantity int4 NOT NULL,\n",
    "    price float8 NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS products (\n",
    "    product_id int4 NOT NULL,\n",
    "    product_name text NOT NULL,\n",
    "    category text NOT NULL,\n",
    "    CONSTRAINT pk_products PRIMARY KEY (product_id)\n",
    ");\n",
    "\n",
    "\n",
    "-- \n",
    "\n",
    "CREATE TABLE IF NOT EXISTS product_analytics_monthly (\n",
    "    product_id int4 NOT NULL,\n",
    "    total_quantity int8 NOT NULL,\n",
    "    total_revenue float8 NOT NULL,\n",
    "    order_count int8 NOT NULL,\n",
    "    avg_rating float8,\n",
    "    positive_reviews int8 NOT NULL,\n",
    "    negative_reviews int8 NOT NULL,\n",
    "    total_reviews int8 NOT NULL,\n",
    "    processing_date date NOT NULL,\n",
    "    CONSTRAINT pk_product_analytics_monthly PRIMARY KEY (processing_date, product_id)\n",
    ");\n",
    "\n",
    "\n",
    "-- 2. для повторных прогонов\n",
    "\n",
    "TRUNCATE TABLE order_items;\n",
    "TRUNCATE TABLE orders;\n",
    "TRUNCATE TABLE products;\n",
    "TRUNCATE TABLE product_analytics_monthly;\n",
    "\n",
    "\n",
    "\n",
    "-- 3. временгные таблицы для загрузки данных:\n",
    "\n",
    "\n",
    "-- orders.csv\n",
    "\n",
    "CREATE TEMP TABLE _stg_orders (order_id int4, customer_id int4, order_date_txt text);\n",
    "\n",
    "\\echo 'orders.csv:'\n",
    "\\copy _stg_orders(order_id, customer_id, order_date_txt) FROM '/data/orders.csv' WITH (FORMAT csv, HEADER true, DELIMITER ',', QUOTE '\"');\n",
    "\n",
    "INSERT INTO orders(order_id, order_date, customer_id)\n",
    "SELECT order_id, NULLIF(order_date_txt, '')::timestamp AS order_date, customer_id FROM _stg_orders;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-- order_items.csv:\n",
    "\n",
    "CREATE TEMP TABLE _stg_order_items (order_item_id int4, order_id int4, product_id int4, quantity int4, price float8);\n",
    "\n",
    "\\echo 'order_items.csv:'\n",
    "\\copy _stg_order_items(order_item_id, order_id, product_id, quantity, price) FROM '/data/order_items.csv' WITH (FORMAT csv, HEADER true, DELIMITER ',', QUOTE '\"');\n",
    "\n",
    "\n",
    "INSERT INTO order_items(order_id, product_id, quantity, price) SELECT order_id, product_id, quantity, price FROM _stg_order_items;\n",
    "\n",
    "\n",
    "\n",
    "-- products.csv: \n",
    "CREATE TEMP TABLE _stg_products (product_id int4, product_name text, category text, price text);\n",
    "\n",
    "\\echo 'products.csv:'\n",
    "\\copy _stg_products(product_id, product_name, category, price) FROM '/data/products.csv' WITH (FORMAT csv, HEADER true, DELIMITER ',', QUOTE '\"');\n",
    "\n",
    "INSERT INTO products(product_id, product_name, category) SELECT product_id, product_name, category FROM _stg_products;\n",
    "\n",
    "COMMIT;\n",
    "\n",
    "\n",
    "-- проверяем:\n",
    "\n",
    "\\echo 'test :'\n",
    "\n",
    "SELECT * FROM orders ORDER BY order_date DESC LIMIT 5;\n",
    "\n",
    "\\echo 'ура'\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "*x39963@DESKTOP-M5RSHAT:~/mipt/hw_spark$ docker compose exec -T postgres psql -U hw -d hw -f /data/init.sql*\n",
    "\n",
    "![1.4](https://github.com/0x39f963/db-novikov_zachet/images/4.png)\n",
    "\n",
    "\n",
    "# 3. Основной код ДЗ и скриншоты выполнения:\n",
    "\n",
    "**./spark/main.py в репозитории**\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "PG_URL = \"jdbc:postgresql://postgres:5432/hw\"\n",
    "PG_PROPS = {\"user\": \"hw\", \"password\": \"hwpass\", \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "MONGO_URI = \"mongodb://mongo:27017\"\n",
    "MONGO_DB = \"hw\"\n",
    "MONGO_COLL = \"reviews\"\n",
    "\n",
    "\n",
    "# docker compose exec -T postgres psql -U hw -d hw -c \"TRUNCATE TABLE product_analytics_monthly;\"\n",
    "\n",
    "\n",
    "def pg_exec(spark, sql: str) -> None:\n",
    "    jvm = spark._sc._gateway.jvm\n",
    "\n",
    "    # АААААааааа\n",
    "    jvm.org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry.register(\n",
    "        PG_PROPS[\"driver\"]\n",
    "    )\n",
    "\n",
    "    conn = jvm.java.sql.DriverManager.getConnection(\n",
    "        PG_URL, PG_PROPS[\"user\"], PG_PROPS[\"password\"]\n",
    "    )\n",
    "    try:\n",
    "        stmt = conn.createStatement()\n",
    "        stmt.execute(sql)\n",
    "        stmt.close()\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "def main():\n",
    "    spark = (\n",
    "        SparkSession.builder.appName(\"product-analytics-monthly\")\n",
    "        .config(\"spark.mongodb.read.connection.uri\", MONGO_URI)\n",
    "        .config(\"spark.mongodb.read.database\", MONGO_DB)\n",
    "        .config(\"spark.mongodb.read.collection\", MONGO_COLL)\n",
    "        .config(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    spark.sql(\"SET spark.sql.session.timeZone=UTC\")\n",
    "\n",
    "    # 1. читаем из постгри (jdbc)\n",
    "    orders_df = spark.read.jdbc(PG_URL, \"orders\", properties=PG_PROPS).select(\n",
    "        F.col(\"order_id\").cast(\"int\"),\n",
    "        F.col(\"customer_id\").cast(\"int\"),\n",
    "        F.col(\"order_date\").cast(\"timestamp\"),\n",
    "    )\n",
    "    orders_df.describe()\n",
    "\n",
    "    order_items_df = spark.read.jdbc(PG_URL, \"order_items\", properties=PG_PROPS).select(\n",
    "        F.col(\"order_id\").cast(\"int\"),\n",
    "        F.col(\"product_id\").cast(\"int\"),\n",
    "        F.col(\"quantity\").cast(\"int\"),\n",
    "        F.col(\"price\").cast(\"double\"),\n",
    "    )\n",
    "    print(order_items_df)\n",
    "\n",
    "    # 2. последний месяц\n",
    "    # !!! LOOK: в чате писали, что сентябрь можно убрать / что это заапрувлено\n",
    "    period = orders_df.select(\n",
    "        F.max(\"order_date\").alias(\"max_dt\"),\n",
    "        F.add_months(F.date_trunc(\"month\", F.max(\"order_date\")), -1).alias(\"ms\"),\n",
    "        F.date_trunc(\"month\", F.max(\"order_date\")).alias(\"me\"),\n",
    "    ).first()\n",
    "\n",
    "    max_dt = period[\"max_dt\"]\n",
    "    ms = period[\"ms\"]\n",
    "    me = period[\"me\"]\n",
    "\n",
    "    print(f\"max_dt: {max_dt} ms:{ms}, me:{me}\")\n",
    "\n",
    "    # 3. фильтр по посл месяцу\n",
    "    orders_m = orders_df.where(\n",
    "        (F.col(\"order_date\") >= F.lit(ms)) & (F.col(\"order_date\") < F.lit(me))\n",
    "    )\n",
    "    sales = order_items_df.join(\n",
    "        orders_m.select(\"order_id\"), on=\"order_id\", how=\"inner\"\n",
    "    ).select(\"order_id\", \"product_id\", \"quantity\", \"price\")\n",
    "\n",
    "    # 4. отзывы, отфильтрованные по посл месяцу\n",
    "    reviews_df = (\n",
    "        spark.read.format(\"mongodb\")\n",
    "        .load()\n",
    "        .select(\n",
    "            F.col(\"product_id\").cast(\"int\").alias(\"product_id\"),\n",
    "            F.col(\"rating\").cast(\"int\").alias(\"rating\"),\n",
    "            F.col(\"created_at\").cast(\"timestamp\").alias(\"created_at\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    reviews_m = reviews_df.where(\n",
    "        (F.col(\"created_at\") >= F.lit(ms)) & (F.col(\"created_at\") < F.lit(me))\n",
    "    )\n",
    "\n",
    "    # 5. temp view для агрегации\n",
    "    sales.createOrReplaceTempView(\"sales\")\n",
    "    reviews_m.createOrReplaceTempView(\"reviews_m\")\n",
    "\n",
    "    sales_agg = spark.sql(\n",
    "        \"\"\"\n",
    "        SELECT\n",
    "          product_id,\n",
    "          COUNT(DISTINCT order_id)      AS order_count,\n",
    "          SUM(CAST(quantity AS BIGINT)) AS total_quantity,\n",
    "          SUM(quantity * price)         AS total_revenue\n",
    "        FROM sales\n",
    "        GROUP BY product_id\n",
    "    \"\"\"\n",
    "    )\n",
    "    print(\"sales_agg:\")\n",
    "    sales_agg.orderBy(\"product_id\").show(50, truncate=False) # p.s. в таблице product у нас 50 записей\n",
    "\n",
    "    reviews_agg = spark.sql(\n",
    "        \"\"\"\n",
    "        SELECT\n",
    "          product_id,\n",
    "          AVG(CAST(rating AS DOUBLE)) AS avg_rating,\n",
    "          COUNT(1)                    AS total_reviews,\n",
    "          SUM(CASE WHEN rating IN (4,5) THEN 1 ELSE 0 END) AS positive_reviews,\n",
    "          SUM(CASE WHEN rating IN (1,2) THEN 1 ELSE 0 END) AS negative_reviews\n",
    "        FROM reviews_m\n",
    "        GROUP BY product_id\n",
    "    \"\"\"\n",
    "    )\n",
    "    print(\"reviews_agg:\")\n",
    "    reviews_agg.orderBy(\"product_id\").show(50, truncate=False)\n",
    "\n",
    "    # 6. объединяем\n",
    "    result = (\n",
    "        sales_agg.join(reviews_agg, on=\"product_id\", how=\"full\")\n",
    "        .na.fill(\n",
    "            {\n",
    "                \"order_count\": 0,\n",
    "                \"total_quantity\": 0,\n",
    "                \"total_revenue\": 0.0,\n",
    "                \"total_reviews\": 0,\n",
    "                \"positive_reviews\": 0,\n",
    "                \"negative_reviews\": 0,\n",
    "            }\n",
    "        )\n",
    "        .withColumn(\"processing_date\", F.to_date(F.lit(ms)))\n",
    "        .select(\n",
    "            \"product_id\",\n",
    "            \"total_quantity\",\n",
    "            \"total_revenue\",\n",
    "            \"order_count\",\n",
    "            \"avg_rating\",\n",
    "            \"positive_reviews\",\n",
    "            \"negative_reviews\",\n",
    "            \"total_reviews\",\n",
    "            \"processing_date\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"result:\")\n",
    "    result.orderBy(\"product_id\").show(50, truncate=False) \n",
    "\n",
    "    # 7. записываем результаты \n",
    "\n",
    "    pg_exec(spark, \"TRUNCATE TABLE product_analytics_monthly;\")\n",
    "\n",
    "    result.write.mode(\"append\").jdbc(\n",
    "        PG_URL, \"product_analytics_monthly\", properties=PG_PROPS\n",
    "    )\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "**3.1**\n",
    "![3.1](https://github.com/0x39f963/db-novikov_zachet/images/6.png)\n",
    "\n",
    ".\n",
    "\n",
    "**3.2**\n",
    "![3.2](https://github.com/0x39f963/db-novikov_zachet/images/7.png)\n",
    "\n",
    ".\n",
    "\n",
    "**3.3**\n",
    "![3.3](https://github.com/0x39f963/db-novikov_zachet/images/8.png)\n",
    "\n",
    ".\n",
    "\n",
    "**3.3**\n",
    "![3.3](https://github.com/0x39f963/db-novikov_zachet/images/9.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcf7778-2dc7-430a-97f3-13fc20d40f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
